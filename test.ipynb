{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b1bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from flask import Flask, request, jsonify\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = pd.read_csv('orders.csv')\n",
    "users_df = pd.read_csv('users.csv')\n",
    "payments_df = pd.read_csv('payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb3e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Data Processor\n",
    "# ---------------------------\n",
    "class DataProcessor:\n",
    "    def __init__(self, order_data_path, user_data_path, payment_data_path):\n",
    "        self.order_data_path = order_data_path\n",
    "        self.user_data_path = user_data_path\n",
    "        self.payment_data_path = payment_data_path\n",
    "\n",
    "        self.node_mappings = {}\n",
    "        self.edge_indices = {}\n",
    "        self.node_features = {}\n",
    "        self.labels = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load raw data from CSV files.\"\"\"\n",
    "        self.orders_df = pd.read_csv(self.order_data_path)\n",
    "        self.users_df = pd.read_csv(self.user_data_path)\n",
    "        self.payments_df = pd.read_csv(self.payment_data_path)\n",
    "        if 'is_fraud' not in self.orders_df.columns:\n",
    "            raise ValueError(\"Order data must have 'is_fraud' column\")\n",
    "\n",
    "    def create_node_mappings(self):\n",
    "        \"\"\"Create mappings from original IDs to consecutive indices.\"\"\"\n",
    "        # Convert all IDs to strings for consistency\n",
    "        unique_users = self.users_df['user_id'].astype(str).unique()\n",
    "        self.node_mappings['user'] = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "\n",
    "        unique_orders = self.orders_df['order_id'].astype(str).unique()\n",
    "        self.node_mappings['order'] = {oid: idx for idx, oid in enumerate(unique_orders)}\n",
    "\n",
    "        unique_payments = self.payments_df['payment_id'].astype(str).unique()\n",
    "        self.node_mappings['payment'] = {pid: idx for idx, pid in enumerate(unique_payments)}\n",
    "\n",
    "    def extract_node_features(self):\n",
    "        \"\"\"Extract and store features for each node type as tensors.\"\"\"\n",
    "        # For users: modify columns as needed\n",
    "        user_features = self.users_df[['age', 'account_age_days', 'total_past_orders', 'avg_order_value']].values\n",
    "        self.node_features['user'] = torch.tensor(user_features, dtype=torch.float)\n",
    "\n",
    "        # For orders: modify columns as needed\n",
    "        order_features = self.orders_df[['order_amount', 'num_items', 'time_of_day', 'day_of_week']].values\n",
    "        self.node_features['order'] = torch.tensor(order_features, dtype=torch.float)\n",
    "\n",
    "        # For payments: use one-hot encoding for payment_type\n",
    "        payment_types = pd.get_dummies(self.payments_df['payment_type']).values\n",
    "        self.node_features['payment'] = torch.tensor(payment_types, dtype=torch.float)\n",
    "\n",
    "        # For orders, we also store fraud labels.\n",
    "        self.labels['order'] = torch.tensor(self.orders_df['is_fraud'].values, dtype=torch.long)\n",
    "\n",
    "    def create_edge_indices(self):\n",
    "        \"\"\"Create edge indices (relationships) between node types.\"\"\"\n",
    "        # User places Order edges\n",
    "        user_order_df = self.orders_df[['user_id', 'order_id']].drop_duplicates()\n",
    "        user_indices = [self.node_mappings['user'][str(uid)] for uid in user_order_df['user_id']]\n",
    "        order_indices = [self.node_mappings['order'][str(oid)] for oid in user_order_df['order_id']]\n",
    "        self.edge_indices[('user', 'places', 'order')] = torch.tensor([user_indices, order_indices])\n",
    "        self.edge_indices[('order', 'placed_by', 'user')] = torch.tensor([order_indices, user_indices])\n",
    "\n",
    "        # Order uses Payment edges\n",
    "        order_payment_df = self.orders_df[['order_id', 'payment_id']].drop_duplicates()\n",
    "        order_indices = [self.node_mappings['order'][str(oid)] for oid in order_payment_df['order_id']]\n",
    "        payment_indices = [self.node_mappings['payment'][str(pid)] for pid in order_payment_df['payment_id']]\n",
    "        self.edge_indices[('order', 'uses', 'payment')] = torch.tensor([order_indices, payment_indices])\n",
    "        self.edge_indices[('payment', 'used_by', 'order')] = torch.tensor([payment_indices, order_indices])\n",
    "\n",
    "    def create_heterograph(self):\n",
    "        \"\"\"Create and return a heterogeneous graph using HeteroData.\"\"\"\n",
    "        self.load_data()\n",
    "        self.create_node_mappings()\n",
    "        self.extract_node_features()\n",
    "        self.create_edge_indices()\n",
    "\n",
    "        data = HeteroData()\n",
    "        # Set node features\n",
    "        for node_type, features in self.node_features.items():\n",
    "            data[node_type].x = features\n",
    "\n",
    "        # Set edge indices\n",
    "        for edge_type, edge_index in self.edge_indices.items():\n",
    "            data[edge_type].edge_index = edge_index\n",
    "\n",
    "        # Set fraud labels for orders\n",
    "        data['order'].y = self.labels['order']\n",
    "\n",
    "        return data\n",
    "\n",
    "    def split_data(self, data, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Split the order nodes into train, validation, and test masks.\"\"\"\n",
    "        num_orders = data['order'].x.size(0)\n",
    "        order_indices = np.arange(num_orders)\n",
    "        # Split into train+val and test\n",
    "        train_val_idx, test_idx = train_test_split(order_indices, test_size=test_size, \n",
    "                                                    stratify=data['order'].y.numpy())\n",
    "        # Split train+val into train and validation\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        train_idx, val_idx = train_test_split(train_val_idx, test_size=val_size_adjusted,\n",
    "                                              stratify=data['order'].y[train_val_idx].numpy())\n",
    "\n",
    "        train_mask = torch.zeros(num_orders, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_orders, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_orders, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "\n",
    "        data['order'].train_mask = train_mask\n",
    "        data['order'].val_mask = val_mask\n",
    "        data['order'].test_mask = test_mask\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e04c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# HGNN Model Definition\n",
    "# ---------------------------\n",
    "class FraudDetectionHGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super(FraudDetectionHGNN, self).__init__()\n",
    "        # metadata is a tuple: (node_feature_dims, edge_types)\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleDict()\n",
    "        for node_type, in_channels in metadata[0].items():\n",
    "            self.embeddings[node_type] = Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # Two HGTConv layers\n",
    "        self.conv1 = HGTConv(hidden_channels, hidden_channels, metadata, num_heads=4, group='sum')\n",
    "        self.conv2 = HGTConv(hidden_channels, hidden_channels, metadata, num_heads=4, group='sum')\n",
    "\n",
    "        # Output layer for the \"order\" node\n",
    "        self.output = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Compute initial embeddings\n",
    "        h_dict = {node_type: self.embeddings[node_type](x) for node_type, x in x_dict.items()}\n",
    "\n",
    "        # First conv layer and activation\n",
    "        h_dict = self.conv1(h_dict, edge_index_dict)\n",
    "        h_dict = {ntype: F.leaky_relu(h) for ntype, h in h_dict.items()}\n",
    "\n",
    "        # Second conv layer and activation\n",
    "        h_dict = self.conv2(h_dict, edge_index_dict)\n",
    "        h_dict = {ntype: F.leaky_relu(h) for ntype, h in h_dict.items()}\n",
    "\n",
    "        # Use the \"order\" node embedding for fraud prediction\n",
    "        out = self.output(h_dict['order'])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f592765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Training Pipeline\n",
    "# ---------------------------\n",
    "class FraudDetectionTrainer:\n",
    "    def __init__(self, model, data, device=None):\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.data = data.to(self.device)\n",
    "\n",
    "        # Define the optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "        # Addressing class imbalance: compute pos_weight for BCE loss\n",
    "        order_labels = self.data['order'].y\n",
    "        pos_weight = (order_labels == 0).sum() / (order_labels == 1).sum()\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def train(self, epochs=100, patience=10):\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(\n",
    "                {node_type: self.data[node_type].x for node_type in self.data.node_types},\n",
    "                {edge_type: self.data[edge_type].edge_index for edge_type in self.data.edge_types}\n",
    "            )\n",
    "            train_mask = self.data['order'].train_mask\n",
    "            loss = self.criterion(out[train_mask].squeeze(), self.data['order'].y[train_mask].float())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_loss = self.evaluate(mode='val')\n",
    "            print(f'Epoch: {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "                torch.save(self.model.state_dict(), 'best_fraud_model.pt')\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                    break\n",
    "\n",
    "    def evaluate(self, mode='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(\n",
    "                {node_type: self.data[node_type].x for node_type in self.data.node_types},\n",
    "                {edge_type: self.data[edge_type].edge_index for edge_type in self.data.edge_types}\n",
    "            )\n",
    "            mask = self.data['order'].val_mask if mode == 'val' else self.data['order'].test_mask\n",
    "            loss = self.criterion(out[mask].squeeze(), self.data['order'].y[mask].float())\n",
    "            if mode == 'test':\n",
    "                preds = torch.sigmoid(out[mask].squeeze()).cpu().numpy()\n",
    "                labels = self.data['order'].y[mask].cpu().numpy()\n",
    "                auc = roc_auc_score(labels, preds)\n",
    "                ap = average_precision_score(labels, preds)\n",
    "                precision, recall, thresholds = precision_recall_curve(labels, preds)\n",
    "                f1_scores = 2 * recall * precision / (recall + precision + 1e-6)\n",
    "                optimal_idx = np.argmax(f1_scores)\n",
    "                optimal_threshold = thresholds[optimal_idx] if thresholds.size > 0 else 0.5\n",
    "                print(f'Test Loss: {loss.item():.4f}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
    "                print(f'Optimal threshold: {optimal_threshold:.4f}')\n",
    "            return loss.item()\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load('best_fraud_model.pt'))\n",
    "        return self.evaluate(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "913cbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Inference API\n",
    "# ---------------------------\n",
    "class FraudDetectionAPI:\n",
    "    def __init__(self, model_path, data_processor, hidden_channels=64, threshold=0.5):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.data_processor = data_processor\n",
    "\n",
    "        # Create a sample heterogeneous graph to extract metadata\n",
    "        sample_data = self.data_processor.create_heterograph()\n",
    "        metadata = (\n",
    "            {node_type: sample_data[node_type].x.size(1) for node_type in sample_data.node_types},\n",
    "            sample_data.edge_types\n",
    "        )\n",
    "        self.model = FraudDetectionHGNN(hidden_channels=hidden_channels, out_channels=1, metadata=metadata).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def process_new_order(self, order_data):\n",
    "        \"\"\"\n",
    "        Convert new order JSON data to a mini-graph, perform inference,\n",
    "        and return the fraud probability along with classification.\n",
    "        \"\"\"\n",
    "        graph_data = self._convert_order_to_graph(order_data)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(\n",
    "                {node_type: graph_data[node_type].x.to(self.device) for node_type in graph_data.node_types},\n",
    "                {edge_type: graph_data[edge_type].edge_index.to(self.device) for edge_type in graph_data.edge_types}\n",
    "            )\n",
    "            # Assume new order is the only order node and at index 0\n",
    "            fraud_prob = torch.sigmoid(out[0]).item()\n",
    "            is_fraud = fraud_prob >= self.threshold\n",
    "        return {\n",
    "            'fraud_probability': fraud_prob,\n",
    "            'is_fraud': is_fraud,\n",
    "            'order_id': order_data['order_id']\n",
    "        }\n",
    "\n",
    "    def _convert_order_to_graph(self, order_data):\n",
    "        \"\"\"\n",
    "        Convert a single new order into a minimal HeteroData graph.\n",
    "        This simplified implementation creates one node for each type.\n",
    "        \"\"\"\n",
    "        data = HeteroData()\n",
    "        # Create user node feature vector â€“ adjust features as needed\n",
    "        data['user'].x = torch.tensor([[\n",
    "            order_data['user_age'],\n",
    "            order_data['account_age_days'],\n",
    "            order_data['user_total_orders'],\n",
    "            order_data['user_avg_order_value']\n",
    "        ]], dtype=torch.float)\n",
    "        # Create order node feature vector\n",
    "        data['order'].x = torch.tensor([[\n",
    "            order_data['order_amount'],\n",
    "            order_data['num_items'],\n",
    "            order_data['time_of_day'],\n",
    "            order_data['day_of_week']\n",
    "        ]], dtype=torch.float)\n",
    "        # Create payment node feature vector using one-hot encoding (assumes payment_type_idx is provided)\n",
    "        # Here we assume a fixed dimensionality (e.g., 4) for payment one-hot vector; adjust as needed.\n",
    "        payment_dim = 4\n",
    "        payment_feature = torch.zeros((1, payment_dim))\n",
    "        payment_idx = int(order_data['payment_type_idx'])\n",
    "        if payment_idx < payment_dim:\n",
    "            payment_feature[0, payment_idx] = 1\n",
    "        data['payment'].x = payment_feature\n",
    "\n",
    "        # Create simple edge indices for the mini-graph (only one edge per relation)\n",
    "        data[('user', 'places', 'order')].edge_index = torch.tensor([[0], [0]])\n",
    "        data[('order', 'placed_by', 'user')].edge_index = torch.tensor([[0], [0]])\n",
    "        data[('order', 'uses', 'payment')].edge_index = torch.tensor([[0], [0]])\n",
    "        data[('payment', 'used_by', 'order')].edge_index = torch.tensor([[0], [0]])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b591a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Flask API Setup\n",
    "# ---------------------------\n",
    "def create_fraud_detection_app(model_path, data_processor):\n",
    "    app = Flask(__name__)\n",
    "    fraud_api = FraudDetectionAPI(model_path, data_processor)\n",
    "\n",
    "    @app.route('/predict', methods=['POST'])\n",
    "    def predict_fraud():\n",
    "        try:\n",
    "            order_data = request.get_json()\n",
    "            required_fields = [\n",
    "                'order_id', 'user_id', 'payment_id', 'order_amount', 'num_items',\n",
    "                'time_of_day', 'day_of_week', 'user_age', 'account_age_days',\n",
    "                'user_total_orders', 'user_avg_order_value', 'payment_type_idx'\n",
    "            ]\n",
    "            for field in required_fields:\n",
    "                if field not in order_data:\n",
    "                    return jsonify({'error': f'Missing required field: {field}'}), 400\n",
    "\n",
    "            result = fraud_api.process_new_order(order_data)\n",
    "            return jsonify(result)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254f96f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\orders.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     app\u001b[38;5;241m.\u001b[39mrun(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.0.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize DataProcessor\u001b[39;00m\n\u001b[0;32m     11\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m DataProcessor(order_data_path, user_data_path, payment_data_path)\n\u001b[1;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_heterograph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m data_processor\u001b[38;5;241m.\u001b[39msplit_data(data)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define metadata for HGNN: node feature dimensions and edge types.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m, in \u001b[0;36mDataProcessor.create_heterograph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_heterograph\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create and return a heterogeneous graph using HeteroData.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node_mappings()\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_node_features()\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mDataProcessor.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load raw data from CSV files.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morders_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_data_path)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayments_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayment_data_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\orders.csv'"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Main Application\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # File paths: update these paths based on your environment.\n",
    "    order_data_path = os.path.join('data', 'orders.csv')\n",
    "    user_data_path = os.path.join('data', 'users.csv')\n",
    "    payment_data_path = os.path.join('data', 'payments.csv')\n",
    "\n",
    "    # Initialize DataProcessor\n",
    "    data_processor = DataProcessor(order_data_path, user_data_path, payment_data_path)\n",
    "    data = data_processor.create_heterograph()\n",
    "    data = data_processor.split_data(data)\n",
    "\n",
    "    # Define metadata for HGNN: node feature dimensions and edge types.\n",
    "    metadata = (\n",
    "        {node_type: data[node_type].x.size(1) for node_type in data.node_types},\n",
    "        data.edge_types\n",
    "    )\n",
    "\n",
    "    # Initialize and train the HGNN model\n",
    "    hidden_channels = 64\n",
    "    model = FraudDetectionHGNN(hidden_channels=hidden_channels, out_channels=1, metadata=metadata)\n",
    "    trainer = FraudDetectionTrainer(model, data)\n",
    "    trainer.train(epochs=100, patience=10)\n",
    "    trainer.test()  # Evaluate on the test set\n",
    "\n",
    "    # Start the Flask API for real-time fraud detection\n",
    "    app = create_fraud_detection_app('best_fraud_model.pt', data_processor)\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5b151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haziq siddiqui\n"
     ]
    }
   ],
   "source": [
    "print(\"Haziq siddiqui\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
